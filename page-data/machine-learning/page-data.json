{"componentChunkName":"component---src-templates-article-tsx","path":"/machine-learning/","result":{"data":{"markdownRemark":{"html":"<h1>Machine Learning (ML)</h1>\n<p>Machine Learning (ML) is the science and practice of creating algorithms that improve through experience.</p>\n<h2>Machine Learning Terminology</h2>\n<table>\n<thead>\n<tr>\n<th>Word</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Data sampling</td>\n<td>Systematic creation of smaller representative samples of larger data sets</td>\n</tr>\n<tr>\n<td>Feature</td>\n<td>A variable with high relevancy to the outcome variable</td>\n</tr>\n<tr>\n<td>Feature selection</td>\n<td>Automatic detection of variables most relevant to the outcome variable</td>\n</tr>\n<tr>\n<td>Imputation</td>\n<td>Correction of corrupt and missing values through inference</td>\n</tr>\n<tr>\n<td>Integer encoding</td>\n<td>Assignment of an integer value to a categorical value, e.g. values \"red\", \"green\", and \"blue\" could be assigned integer values of 1, 2, and 3 respectively</td>\n</tr>\n<tr>\n<td>One-hot encoding</td>\n<td>Assignment of a bit-mapped binary value to a set of categorical values, e.g. a \"color\" category with potential values of \"red\", \"green\", and \"blue\" could be mapped to three bits of 100, 010, and 001, respectively</td>\n</tr>\n<tr>\n<td>Outcome variable</td>\n<td>The value to be predicted by a Machine Learning Model</td>\n</tr>\n<tr>\n<td>Outlier</td>\n<td>A observation significantly different from other observations of the same data</td>\n</tr>\n</tbody>\n</table>\n<h2>The Machine Learning Process</h2>\n<div class=\"mermaid\">graph TD\n    subgraph 1. Source the Data\n      DB1[(Data)] --> Gather\n      DB2[(Data)] --> Gather\n      DB3[(Data)] -->\n      Gather --> Raw[(Raw Data)]\n    end\n\t\tsubgraph 2. Wrangle the Data\n      Raw --> Understand\n      Understand -->|Work with SMEs| Summarize\n      Understand --> Visualize\n\t\t\tSummarize --> Cleanse\n      Visualize -->\n      Cleanse -->|Imputation, Outlier Detection...| Cleansed[(Cleansed Data)]\n      Cleansed -->\n      Select -.->|Gather identified missing data| Gather\n      Select --> Sample[Data Sampling]\n      Select --> Features[Feature Selection]\n      Sample --> Prepare\n      Features --> Prepare\n      Prepare --> Encode[Encode Categorical Data]\n      Prepare --> Normalize\n      Encode --> Data[(Prepared Data)]\n      Normalize --> Data\n\t\tend\n    subgraph 3. Model the Data\n      Data --> Model\n    end\n    subgraph 4. Use the Model\n      Model --> Use[Use the Model]\n    end</div>\n<h3>1. Source the Data</h3>\n<p>Work with Subject Matter Experts (SMEs) to identify and gather the relevant data sources.</p>\n<h3>2. Wrangle the Data</h3>\n<ol>\n<li><strong>Understand:</strong> Defining the meaning and relationships for each field. Summary statistics, data visualizations, and guidance from SMEs are used to scrutinize the data.</li>\n<li><strong>Cleanse:</strong> Detect and address corrupt and missing data. Outlier detection and imputation are used to cleanse the data.</li>\n<li><strong>Select:</strong> Remove unneeded data from the data set, and gather missing data that cannot be reliably inferred. Data sampling is used to systematically create smaller representative samples of larger datasets and feature selection is used to automatically identify the variables most relevant to the outcome variable.</li>\n<li><strong>Prepare:</strong> Standardize and normalize the data into a consistent structure and format. Integer encoding and one-hot encoding are used to convert categorical data to numerical data to make it easier for a machine learning model to process.</li>\n</ol>\n<h4>Outlier Detection</h4>\n<p>The most common approaches to outlier detection are to use <a href=\"../standard-deviation/\">standard deviation (STD)</a> or <a href=\"../interquartile-range/\">interquartile range (IQR)</a>.</p>\n<p>The following steps are a simple example using standard deviation, but interquartile-range could be used just as easily:</p>\n<ol>\n<li>Calculate the mean and standard deviation (alternatively, interquartile range) of the data collection</li>\n<li>Set a cutoff (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">c</span></span></span></span>) of three standard deviations (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>), or <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi><mo>=</mo><mn>3</mn><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">c = 3\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span></li>\n<li>Set a lower-bound (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span></span></span></span>) of the mean minus the cutoff (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mo>=</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"true\">‾</mo></mover><mo>−</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">l = \\overline{x} - c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.71389em;vertical-align:-0.08333em;\"></span><span class=\"mord overline\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.63056em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.55056em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"overline-line\" style=\"border-bottom-width:0.04em;\"></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">c</span></span></span></span>) and an upper-bound (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi></mrow><annotation encoding=\"application/x-tex\">u</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">u</span></span></span></span>) of the mean plus the cutoff (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi><mo>=</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"true\">‾</mo></mover><mo>+</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">u = \\overline{x} + c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">u</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.71389em;vertical-align:-0.08333em;\"></span><span class=\"mord overline\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.63056em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.55056em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"overline-line\" style=\"border-bottom-width:0.04em;\"></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">c</span></span></span></span>)</li>\n</ol>\n<p>All data points less than the lower-bound or greater than the upper-bound can be considered outliers.</p>\n<h4>Imputation Options</h4>\n<p>The basic options for imputation are to do nothing, remove records with missing/corrupt values, or replace the missing/corrupt values (usually with the mean or mode value), or some combination of these options. It's generally best to test each option and compare the outcomes to determine the best approach.</p>\n<h3>3. Model the Data</h3>\n<p>I'm still learning! This section will be updated as I learn. If you have a good resource to suggestion, please <a href=\"https://github.com/jamestharpe/jamestharpe.com/issues/new\">tell me about it</a>.</p>\n<h3>4. Use the Model</h3>\n<p>I'm still learning! This section will be updated as I learn. If you have a good resource to suggestion, please <a href=\"https://github.com/jamestharpe/jamestharpe.com/issues/new\">tell me about it</a>.</p>\n<h2>Tools for Machine Learning</h2>\n<ul>\n<li><a href=\"https://spark.apache.org/\">Apache Spark</a></li>\n<li><a href=\"https://azure.microsoft.com/en-us/services/machine-learning/\">Azure Machine Learning</a></li>\n<li><a href=\"https://pandas.pydata.org/\">Pandas</a></li>\n</ul>","fields":{"slug":"/machine-learning/"},"frontmatter":{"title":"Machine Learning (ML)","tags":["statistics"]}}},"pageContext":{"slug":"/machine-learning/"}},"staticQueryHashes":["2270328656","3794076007","80858887"]}