{"componentChunkName":"component---src-templates-article-tsx","path":"/gpt/","result":{"data":{"mdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"date\": \"2023-03-17T17:43:52.000Z\",\n  \"description\": \"An architecture to use unsupervised learning to generate human-like language\",\n  \"tags\": [\"ml-unsupervised\"],\n  \"title\": \"Generative Pre-trained Transformer (GPT)\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Generative Pre-trained Transformer (GPT)\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generative Pre-trained Transformer (GPT)\"), \" is a type of neural network architecture that uses \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"../ml-unsupervised/\"\n  }, \"unsupervised learning\"), \" to generate human-like language. The GPT model was introduced in a 2018 paper by OpenAI titled \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\"\n  }, \"Improving Language Understanding by Generative Pre-Training\"), \".\"), mdx(\"p\", null, \"Models that use GPT architecture are pre-trained on a enormous amounts of text data, such as Wikipedia and countless other websites. During training, the model learns to predict missing words in a sentence based on the context of the surrounding words. Once a model is trained, it can be fine-tuned on specific tasks such as language translation, question answering, and text classification. This allows the model to adapt its language generation capabilities to the specific requirements of various downstream tasks.\"));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/gpt/"},"frontmatter":{"description":"An architecture to use unsupervised learning to generate human-like language","tags":["ml-unsupervised"],"title":"Generative Pre-trained Transformer (GPT)"}}},"pageContext":{"slug":"/gpt/"}},"staticQueryHashes":["3772659826","3794076007","80858887"]}