---
date: 2023-03-17T13:43:52-04:00
description: "An architecture to use unsupervised learning to generate human-like language"
tags: ["ml-unsupervised"]
title: "Generative Pre-trained Transformer (GPT)"
---

# Generative Pre-trained Transformer (GPT)

**Generative Pre-trained Transformer (GPT)** is a type of neural network architecture that uses [unsupervised learning](ml-unsupervised.md) to generate human-like language. The GPT model was introduced in a 2018 paper by OpenAI titled [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).

Models that use GPT architecture are pre-trained on a enormous amounts of text data, such as Wikipedia and countless other websites. During training, the model learns to predict missing words in a sentence based on the context of the surrounding words. Once a model is trained, it can be fine-tuned on specific tasks such as language translation, question answering, and text classification. This allows the model to adapt its language generation capabilities to the specific requirements of various downstream tasks.
